import os
import logging
import pickle
import numpy as np
import requests
import torch
from sentence_transformers import SentenceTransformer
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
# Note: Les imports pour la recherche par image (ex: PIL, scipy) sont laissÃ©s en commentaire.

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- VARIABLES GLOBALES ---
# Variables pour la recherche par image (Vecteurs pour la recherche visuelle)
features_array = None 
image_ids = NoneÂ  Â  Â  
metadata_list = NoneÂ  
IMAGE_EMBEDDINGS_PATH = 'data/embeddings.pkl' 

# Variables pour la recherche textuelle GRATUITE (Vecteurs pour la recherche sÃ©mantique)
text_features_array = NoneÂ  
text_metadata_list = NoneÂ  Â 
TEXT_EMBEDDING_MODEL = None 
MODEL_NAME_FREE = "all-MiniLM-L6-v2"
TEXT_EMBEDDINGS_PATH = 'data/text_embeddings_free.pkl'

# --- FONCTION DE CHARGEMENT DE BASE DE DONNÃ‰ES ---

def load_database():
Â  Â  """Chargement de toutes les bases de donnÃ©es (Image et Texte) au dÃ©marrage de l'API."""
Â  Â  # DÃ©clarez toutes les variables globales que vous modifiez
Â  Â  global features_array, image_ids, metadata_list, text_features_array, text_metadata_list, TEXT_EMBEDDING_MODEL
Â  Â Â 
Â  Â  logger.info("ğŸš€ DÃ‰BUT CHARGEMENT BASE DE DONNÃ‰ES")
Â  Â Â 
Â  Â  # 1. CHARGEMENT BASE D'IMAGES (Si vous avez vos fichiers)
Â  Â  if os.path.exists(IMAGE_EMBEDDINGS_PATH):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“¦ Chargement {IMAGE_EMBEDDINGS_PATH}...")
            # --- Votre logique de chargement d'embeddings d'images ici ---
            # Exemple:
            # with open(IMAGE_EMBEDDINGS_PATH, 'rb') as f:
            #     data = pickle.load(f)
            #     features_array = np.array(data['features'])
            #     image_ids = data['image_ids']
            #     metadata_list = data['metadata']
Â  Â  Â  Â  Â  Â  logger.info("âœ… Embeddings Images chargÃ©s. (VÃ©rifiez les logs pour la forme si implÃ©mentÃ©)")
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Erreur chargement embeddings images: {str(e)}")

Â  Â  # 2. CHARGEMENT BASE TEXTUELLE GRATUITE (NÃ‰CESSAIRE pour /api/search_text)
Â  Â  if os.path.exists(TEXT_EMBEDDINGS_PATH):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ“¦ Chargement {TEXT_EMBEDDINGS_PATH}...")
Â  Â  Â  Â  Â  Â  with open(TEXT_EMBEDDINGS_PATH, 'rb') as f:
Â  Â  Â  Â  Â  Â  Â  Â  data = pickle.load(f)
Â  Â  Â  Â  Â  Â  Â  Â  text_features_array = np.array(data['features'])
Â  Â  Â  Â  Â  Â  Â  Â  text_metadata_list = data['metadata']
Â  Â  Â  Â  Â  Â  logger.info(f"âœ… Embeddings Texte Gratuits chargÃ©s: {text_features_array.shape}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Charger le modÃ¨le pour les requÃªtes utilisateur
Â  Â  Â  Â  Â  Â  logger.info(f"ğŸ§  Chargement du modÃ¨le {MODEL_NAME_FREE} pour les requÃªtes utilisateur...")
Â  Â  Â  Â  Â  Â  device = 'cuda' if torch.cuda.is_available() else 'cpu'Â 
Â  Â  Â  Â  Â  Â  TEXT_EMBEDDING_MODEL = SentenceTransformer(MODEL_NAME_FREE, device=device)
Â  Â  Â  Â  Â  Â  logger.info(f"âœ… ModÃ¨le de requÃªte Texte chargÃ© sur {device}.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"âŒ Erreur chargement embeddings texte ou modÃ¨le: {str(e)}")
Â  Â  Â  Â  Â  Â  text_features_array = None
Â  Â  Â  Â  Â  Â  text_metadata_list = None
Â  Â  Â  Â  Â  Â  TEXT_EMBEDDING_MODEL = None
Â  Â  else:
Â  Â  Â  Â  logger.warning(f"âš ï¸ FICHIER TEXTUEL GRATUIT MANQUANT: {TEXT_EMBEDDINGS_PATH}. Recherche texte dÃ©sactivÃ©e.")
Â  Â  Â  Â Â 
Â  Â  return True

# --- FONCTIONS DE RECHERCHE TEXTUELLE ---

def extract_text_features(text_query):
Â  Â  # ... (Le corps de cette fonction reste inchangÃ© par rapport Ã  votre version)
Â  Â  global TEXT_EMBEDDING_MODEL
Â  Â Â 
Â  Â  if TEXT_EMBEDDING_MODEL is None:
Â  Â  Â  Â  logger.error("âŒ ModÃ¨le Sentence Transformer non chargÃ©. Impossible de vectoriser la requÃªte.")
Â  Â  Â  Â  return None
Â  Â  Â  Â  Â 
Â  Â  try:
Â  Â  Â  Â  logger.info(f"ğŸ§  Extraction features texte local pour: '{text_query[:30]}...'")
Â  Â  Â  Â Â 
Â  Â  Â  Â  features = TEXT_EMBEDDING_MODEL.encode(
Â  Â  Â  Â  Â  Â  text_query,Â 
Â  Â  Â  Â  Â  Â  normalize_embeddings=True,Â 
Â  Â  Â  Â  Â  Â  convert_to_numpy=True
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  return features.flatten()
Â  Â  Â  Â  Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  logger.error(f"ğŸ’¥ Extraction Texte Error: {str(e)}")
Â  Â  Â  Â  return None

def search_logic_text(query, top_k=10):
Â  Â  # ... (Le corps de cette fonction reste inchangÃ© par rapport Ã  votre version)
Â  Â  global text_features_array, text_metadata_list
Â  Â Â 
Â  Â  if text_features_array is None or TEXT_EMBEDDING_MODEL is None:
Â  Â  Â  Â  return {'success': False, 'error': 'Service de recherche texte non prÃªt (base de donnÃ©es ou modÃ¨le non chargÃ©)'}, 503
Â  Â  Â  Â Â 
Â  Â  # 1. Extraire le vecteur de la requÃªte utilisateur
Â  Â  query_features = extract_text_features(query)
Â  Â  if query_features is None:
Â  Â  Â  Â  return {'success': False, 'error': 'Impossible de vectoriser la requÃªte'}, 500
Â  Â Â 
Â  Â  # 2. Calculer similaritÃ©s
Â  Â  logger.info("ğŸ§® Calcul des similaritÃ©s textuelles...")
Â  Â  Â  Â Â 
Â  Â  if text_features_array.shape[1] != query_features.shape[0]:
Â  Â  Â  Â  logger.error(f"IncompatibilitÃ© de dimension : DB {text_features_array.shape[1]} vs Query {query_features.shape[0]}")
Â  Â  Â  Â  return {'success': False, 'error': 'IncompatibilitÃ© de dimension de vecteur'}, 500
Â  Â  Â  Â Â 
Â  Â  similarity_scores = np.dot(text_features_array, query_features)
Â  Â Â 
Â  Â  # 3. Trier les rÃ©sultats
Â  Â  sorted_indices = np.argsort(similarity_scores)[::-1]
Â  Â Â 
Â  Â  top_results = []
Â  Â  for rank in range(min(top_k, len(sorted_indices))):
Â  Â  Â  Â  i = sorted_indices[rank]
Â  Â  Â  Â  score = similarity_scores[i]
Â  Â  Â  Â Â 
Â  Â  Â  Â  if i >= len(text_metadata_list):
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  metadata = text_metadata_list[i]
Â  Â  Â  Â Â 
Â  Â  Â  Â  top_results.append({
Â  Â  Â  Â  Â  Â  'index': int(i),
Â  Â  Â  Â  Â  Â  'similarity': float(score),
Â  Â  Â  Â  Â  Â  'nom': metadata.get('nom', 'N/A'),
Â  Â  Â  Â  Â  Â  'artiste': metadata.get('artiste', 'N/A'),
Â  Â  Â  Â  Â  Â  'annee': metadata.get('annee', 'N/A'),
Â  Â  Â  Â  Â  Â  'image_url': f"/images/{metadata.get('image_id', '')}",Â 
Â  Â  Â  Â  Â  Â  'lien_site': metadata.get('lien_site', 'N/A')
Â  Â  Â  Â  })

Â  Â  top_scores = [f'{s["similarity"]:.3f}' for s in top_results[:3]]
Â  Â  logger.info("âœ… Top 3 similaritÃ©s texte: {}".format(top_scores))
Â  Â Â 
Â  Â  return {
Â  Â  Â  Â  'success': True,
Â  Â  Â  Â  'query_processed': True,
Â  Â  Â  Â  'total_database_size': len(text_features_array) if text_features_array is not None else 0,
Â  Â  Â  Â  'results': top_results
Â  Â  }, 200


# --- INITIALISATION DE L'APPLICATION FLASK ---

app = Flask(__name__, static_folder='static')
CORS(app)

# âš¡ï¸ CHARGEMENT IMMÃ‰DIAT DES MODÃˆLES
# Gunicorn appellera cette fonction une fois au dÃ©marrage de chaque worker.
load_database()

# --- ROUTES D'API ---

# Route de SantÃ© (Health Check) - Pour vÃ©rifier que l'API est vivante
@app.route('/health', methods=['GET'])
def health_check():
    """VÃ©rifie si le service est prÃªt (modÃ¨les chargÃ©s)."""
    status = 'ok' if text_features_array is not None and TEXT_EMBEDDING_MODEL is not None else 'loading'
    
    if status == 'ok':
        message = 'Service de recherche sÃ©mantique prÃªt.'
    else:
        message = 'Chargement en cours ou Ã©chec du chargement de la base de donnÃ©es/modÃ¨le.'
        logger.warning("âš ï¸ Health Check: ModÃ¨le non prÃªt.")
        
    return jsonify({
        'status': status,
        'message': message,
        'db_ready': text_features_array is not None
    }), 200 if status == 'ok' else 503


# Route pour la recherche sÃ©mantique textuelle (via le chatbot)
@app.route('/api/search_text', methods=['POST', 'OPTIONS'])
def api_search_text():
Â  Â  """ROUTE PRINCIPALE POUR LA RECHERCHE SÃ‰MANTIQUE TEXTUELLE"""
Â  Â  try:
Â  Â  Â  Â  logger.info("ğŸ” API SEARCH TEXT ENDPOINT")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if request.method == 'OPTIONS':
Â  Â  Â  Â  Â  Â  # GÃ¨re le preflight CORS
Â  Â  Â  Â  Â  Â  response = jsonify({'status': 'OK'})
Â  Â  Â  Â  Â  Â  response.headers.add("Access-Control-Allow-Origin", "*")
Â  Â  Â  Â  Â  Â  response.headers.add('Access-Control-Allow-Headers', "*")
Â  Â  Â  Â  Â  Â  response.headers.add('Access-Control-Allow-Methods', "*")
Â  Â  Â  Â  Â  Â  return response
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  data = request.json
Â  Â  Â  Â  query = data.get('query')
Â  Â  Â  Â  top_k = int(data.get('top_k', 10))
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not query:
Â  Â  Â  Â  Â  Â  return jsonify({'success': False, 'error': 'RequÃªte de recherche manquante'}), 400
Â  Â  Â  Â Â 
Â  Â  Â  Â  result, status_code = search_logic_text(query, top_k)
Â  Â  Â  Â  return jsonify(result), status_code
Â  Â  Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  logger.error(f"ğŸ’¥ API Search Text Error: {type(e).__name__}: {str(e)}")
Â  Â  Â  Â  return jsonify({'success': False, 'error': 'Erreur interne du serveur'}), 500

# Route pour servir les images (essentielle pour le frontend)
@app.route('/images/<path:filename>')
def serve_image(filename):
Â  Â  """Sert les images stockÃ©es dans static/images/"""
Â  Â  # Attention: 'static/images' doit Ãªtre le dossier oÃ¹ se trouvent vos images
Â  Â  return send_from_directory('static/images', filename)

# TODO: INSEREZ ICI VOTRE ROUTE DE RECHERCHE PAR IMAGE EXISTANTE (/api/search)
# @app.route('/api/search', methods=['POST', 'OPTIONS'])
# def api_search_image():
#Â  Â  Â # ... Votre code existant pour la recherche par image ...
#Â  Â  Â pass


# --- LANCEMENT DE L'APPLICATION (Pour le test local uniquement) ---
if __name__ == '__main__':
    # Ne devrait pas Ãªtre utilisÃ© par Cloud Run, mais pour le test local
    port = int(os.environ.get("PORT", 5000))
    app.run(debug=True, host='0.0.0.0', port=port)
