import os
import logging
import pickle
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- VARIABLES GLOBALES ---
# Variables pour la recherche par image (Vecteurs pour la recherche visuelle)
features_array = None
image_ids = None
metadata_list = None
IMAGE_EMBEDDINGS_PATH = 'data/embeddings.pkl'

# Variables pour la recherche textuelle GRATUITE (Vecteurs pour la recherche s√©mantique)
text_features_array = None
text_metadata_list = None
TEXT_EMBEDDING_MODEL = None
MODEL_NAME_FREE = "all-MiniLM-L6-v2"
# Le chemin vers le PKL maintenant sur GitHub
TEXT_EMBEDDINGS_PATH = 'data/text_embeddings_free.pkl'

# --- FONCTION DE CHARGEMENT DE BASE DE DONN√âES ---

def load_database():
    """
    Charge les bases de donn√©es (images et texte) depuis les fichiers locaux.
    Ceci est appel√© une fois au d√©marrage par Gunicorn.
    """
    # D√©clarez toutes les variables globales que vous modifiez
    global features_array, image_ids, metadata_list, text_features_array, text_metadata_list, TEXT_EMBEDDING_MODEL

    logger.info("üöÄ D√âBUT CHARGEMENT BASE DE DONN√âES DEPUIS FICHIERS LOCAUX")

    # 1. CHARGEMENT BASE TEXTUELLE CRITIQUE
    if os.path.exists(TEXT_EMBEDDINGS_PATH):
        try:
            logger.info(f"üì¶ Chargement {TEXT_EMBEDDINGS_PATH}...")
            with open(TEXT_EMBEDDINGS_PATH, 'rb') as f:
                data = pickle.load(f)
                text_features_array = np.array(data['features'])
                text_metadata_list = data['metadata']
            logger.info(f"‚úÖ Embeddings Texte Gratuits charg√©s: {text_features_array.shape}")

            # Charger le mod√®le pour les requ√™tes utilisateur (√©tape gourmande en ressources)
            logger.info(f"üß† Chargement du mod√®le {MODEL_NAME_FREE} pour les requ√™tes utilisateur...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            TEXT_EMBEDDING_MODEL = SentenceTransformer(MODEL_NAME_FREE, device=device)
            logger.info(f"‚úÖ Mod√®le de requ√™te Texte charg√© sur {device}.")

        except Exception as e:
            logger.error(f"‚ùå Erreur FATALE lors du chargement ou de l'initialisation du mod√®le: {str(e)}")
            text_features_array = None
            text_metadata_list = None
            TEXT_EMBEDDING_MODEL = None
    else:
        logger.error(f"‚ö†Ô∏è FICHIER TEXTUEL CRITIQUE MANQUANT: {TEXT_EMBEDDINGS_PATH}. L'API de recherche texte est D√âSACTIV√âE.")

    # 2. CHARGEMENT BASE D'IMAGES (Si vous l'utilisez, sinon les variables resteront None)
    if os.path.exists(IMAGE_EMBEDDINGS_PATH):
        logger.info(f"üì¶ Chargement {IMAGE_EMBEDDINGS_PATH}...")
        # (Logique de chargement d'images ici si n√©cessaire)
    else:
        logger.warning(f"‚ö†Ô∏è FICHIER IMAGE MANQUANT: {IMAGE_EMBEDDINGS_PATH}")

    logger.info("üèÅ Fin du chargement de la base de donn√©es.")
    return True

# --- FONCTIONS DE RECHERCHE TEXTUELLE (INCHANG√âES) ---

def extract_text_features(text_query):
    global TEXT_EMBEDDING_MODEL

    if TEXT_EMBEDDING_MODEL is None:
        logger.error("‚ùå Mod√®le Sentence Transformer non charg√©. Impossible de vectoriser la requ√™te.")
        return None

    try:
        logger.info(f"üß† Extraction features texte local pour: '{text_query[:30]}...'")

        features = TEXT_EMBEDDING_MODEL.encode(
            text_query,
            normalize_embeddings=True,
            convert_to_numpy=True
        )

        return features.flatten()

    except Exception as e:
        logger.error(f"üí• Extraction Texte Error: {str(e)}")
        return None

def search_logic_text(query, top_k=10):
    global text_features_array, text_metadata_list

    if text_features_array is None or TEXT_EMBEDDING_MODEL is None:
        return {'success': False, 'error': 'Service de recherche texte non pr√™t (base de donn√©es ou mod√®le non charg√©)'}, 503

    # 1. Extraire le vecteur de la requ√™te utilisateur
    query_features = extract_text_features(query)
    if query_features is None:
        return {'success': False, 'error': 'Impossible de vectoriser la requ√™te'}, 500

    # 2. Calculer similarit√©s
    logger.info("üßÆ Calcul des similarit√©s textuelles...")

    if text_features_array.shape[1] != query_features.shape[0]:
        logger.error(f"Incompatibilit√© de dimension : DB {text_features_array.shape[1]} vs Query {query_features.shape[0]}")
        return {'success': False, 'error': 'Incompatibilit√© de dimension de vecteur'}, 500

    similarity_scores = np.dot(text_features_array, query_features)

    # 3. Trier les r√©sultats
    sorted_indices = np.argsort(similarity_scores)[::-1]

    top_results = []
    for rank in range(min(top_k, len(sorted_indices))):
        i = sorted_indices[rank]
        score = similarity_scores[i]

        if i >= len(text_metadata_list):
            continue

        metadata = text_metadata_list[i]

        top_results.append({
            'index': int(i),
            'similarity': float(score),
            'nom': metadata.get('nom', 'N/A'),
            'artiste': metadata.get('artiste', 'N/A'),
            'annee': metadata.get('annee', 'N/A'),
            'image_url': f"/images/{metadata.get('image_id', '')}",
            'lien_site': metadata.get('lien_site', 'N/A')
        })

    top_scores = [f'{s["similarity"]:.3f}' for s in top_results[:3]]
    logger.info("‚úÖ Top 3 similarit√©s texte: {}".format(top_scores))

    return {
        'success': True,
        'query_processed': True,
        'total_database_size': len(text_features_array) if text_features_array is not None else 0,
        'results': top_results
    }, 200


# --- INITIALISATION DE L'APPLICATION FLASK ---

app = Flask(__name__, static_folder='static')
CORS(app)

# ‚ö°Ô∏è CHARGEMENT IMM√âDIAT DES MOD√àLES
# Gunicorn appellera cette fonction une fois au d√©marrage de chaque worker.
load_database()

# --- ROUTES D'API ---

# Route de Base (DEBUG)
@app.route('/', methods=['GET'])
def home():
    """Route de base simple pour tester si Flask r√©pond."""
    return jsonify({
        'status': 'Bienvenue',
        'message': 'API en ligne. Testez /health ou /api/search_text',
        'db_ready': text_features_array is not None
    })

# Route de Sant√© (Health Check)
@app.route('/health', methods=['GET'])
def health_check():
    """V√©rifie si le service est pr√™t (mod√®les charg√©s)."""
    status = 'ok' if text_features_array is not None and TEXT_EMBEDDING_MODEL is not None else 'loading'

    if status == 'ok':
        message = 'Service de recherche s√©mantique pr√™t.'
    else:
        message = 'Chargement en cours ou √©chec du chargement de la base de donn√©es/mod√®le.'
        logger.warning("‚ö†Ô∏è Health Check: Mod√®le non pr√™t.")

    return jsonify({
        'status': status,
        'message': message,
        'db_ready': text_features_array is not None
    }), 200 if status == 'ok' else 503


# Route pour la recherche s√©mantique textuelle (via le chatbot)
@app.route('/api/search_text', methods=['POST', 'OPTIONS'])
def api_search_text():
    """ROUTE PRINCIPALE POUR LA RECHERCHE S√âMANTIQUE TEXTUELLE"""
    try:
        logger.info("üîç API SEARCH TEXT ENDPOINT")

        if request.method == 'OPTIONS':
            # G√®re le preflight CORS
            response = jsonify({'status': 'OK'})
            response.headers.add("Access-Control-Allow-Origin", "*")
            response.headers.add('Access-Control-Allow-Headers', "*")
            response.headers.add('Access-Control-Allow-Methods', "*")
            return response

        data = request.json
        query = data.get('query')
        top_k = int(data.get('top_k', 10))

        if not query:
            return jsonify({'success': False, 'error': 'Requ√™te de recherche manquante'}), 400

        result, status_code = search_logic_text(query, top_k)
        return jsonify(result), status_code

    except Exception as e:
        logger.error(f"üí• API Search Text Error: {type(e).__name__}: {str(e)}")
        return jsonify({'success': False, 'error': 'Erreur interne du serveur'}), 500

# Route pour servir les images (essentielle pour le frontend)
@app.route('/images/<path:filename>')
def serve_image(filename):
    """Sert les images stock√©es dans static/images/"""
    # ATTENTION: 'static/images' doit √™tre le dossier o√π se trouvent vos images
    return send_from_directory('static/images', filename)


# --- LANCEMENT DE L'APPLICATION (Pour le test local uniquement) ---
if __name__ == '__main__':
    # Ne devrait pas √™tre utilis√© par Cloud Run, mais pour le test local
    port = int(os.environ.get("PORT", 5000))
    app.run(debug=True, host='0.0.0.0', port=port)
